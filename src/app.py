
from langchain_openai import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
import json
import os
import traceback


class App:
    def __init__(self) -> None:
        self.__key: str = None
        self.__client: OpenAI = None
    
    @property
    def key(self) -> str:
        return self.__key
    
    @key.setter
    def key(self, value: str):
        value = value.strip()
        assert value is not None, "The API key cannot be None."
        assert isinstance(value, str), "The API key must be a string."
        assert len(value) > 0, "The API key cannot be an empty string."

        self.__key = value
        self.__client = OpenAI(api_key=self.__key, temperature=.7, max_tokens=1500, max_retries=3)

    def run(self, prompt: str) -> dict:
        try: 
            return self._run(prompt=prompt)
        except Exception as e:
            return {
                "status": "error",
                "message": str(e) + ": " + traceback.format_exc() if os.getenv("logging", "INFO") == "DEBUG" else str(e)
            }
    def _run(self, prompt: str) -> dict:
        assert prompt is not None, "The prompt cannot be None."
        assert isinstance(prompt, str), "The prompt must be a string."
        assert len(prompt) > 0, "The prompt cannot be an empty string."
        assert len(prompt.split()) <= 2500 * 4/3, f"The prompt cannot exceed {2500 * 4/3} words."


        chain: LLMChain = self._build_chain()
        assert chain is not None, "Internal error: 7."

        response = chain.invoke(input={
            "prompt_to_optimize": prompt,
        })

        assert response is not None
        assert response["text"] is not None 

        response = response["text"]
        return {
            "status": "success",
            "optimized_prompt": response["optimized_prompt"],
            "required_info": response["required_info"],
        }
        
    def _build_chain(self) -> LLMChain:
        prompt_dict = json.load(open("./src/data/prompt.json", "r"))
        assert prompt_dict is not None, "Internal error: 1."
        assert isinstance(prompt_dict, dict), "Internal error: 2."
        assert "prompt" in prompt_dict, "Internal error: 3."

        prompt = prompt_dict["prompt"]
        assert prompt is not None, "Internal error: 4."
        assert isinstance(prompt, str), "Internal error: 5."
        assert len(prompt) > 0, "Internal error: 6."

        guidelines_dict = json.load(open("./src/data/guidelines.json", "r"))
        assert guidelines_dict is not None, "Internal error: 8."
        assert isinstance(guidelines_dict, dict), "Internal error: 9."
        assert "guidelines" in guidelines_dict, "Internal error: 10."

        guidelines = guidelines_dict["guidelines"]
        assert guidelines is not None, "Internal error: 11."
        assert isinstance(guidelines, str), "Internal error: 12."
        assert len(guidelines) > 0, "Internal error: 13."

        optimizer_output_parser = StructuredOutputParser(response_schemas=[
            ResponseSchema(name="optimized_prompt", description="This is the optimized prompt that was generated by following the guidelines."),
            ResponseSchema(name="required_info", description="This is the additional information that was required to optimize the prompt. If no additional information was required, respond by saying 'No additional information was required.'."),
        ])
        
        optimizer_format_instructions = optimizer_output_parser.get_format_instructions()
        optimizer_prompt = PromptTemplate(
            template=prompt,
            input_types={
                "format_instructions": str,
                "guidelines": str,
                "prompt_to_optimize": str
            },
            input_variables=["prompt_to_optimize"],
            partial_variables={
                "format_instructions": optimizer_format_instructions,
                "guidelines": guidelines
            },
            output_parser=optimizer_output_parser
        )

        optimizer_chain = LLMChain(
            llm=self.__client,
            output_parser=optimizer_output_parser,
            prompt=optimizer_prompt
        )

        return optimizer_chain